{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUkMcYmB4O6Qj79i/jZ3Nb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yasmeenAdel154/FRONT-END/blob/main/NLP_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**this code from : https://towardsdatascience.com/question-answering-with-a-fine-tuned-bert-bc4dafd45626**"
      ],
      "metadata": {
        "id": "oMAyddqSQCER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq2S3YD4D-9s",
        "outputId": "40da3356-9d8d-48c1-89f7-aa1c01eeb94e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Loading from Local CSV File"
      ],
      "metadata": {
        "id": "GKXHd8D3MZ-E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lda-Vu4kDcT1",
        "outputId": "86906af8-eb01-44f7-fc58-02db0938ddf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0  Ø¬Ù…Ø§Ù„ Ø£Ø­Ù…Ø¯ Ø­Ù…Ø²Ø© Ø®Ø§Ø´Ù‚Ø¬ÙŠ (13 Ø£ÙƒØªÙˆØ¨Ø± 1958ØŒ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©...   \n",
            "1  Ø¬Ù…Ø§Ù„ Ø£Ø­Ù…Ø¯ Ø­Ù…Ø²Ø© Ø®Ø§Ø´Ù‚Ø¬ÙŠ (13 Ø£ÙƒØªÙˆØ¨Ø± 1958ØŒ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©...   \n",
            "2  Ø¬Ù…Ø§Ù„ Ø£Ø­Ù…Ø¯ Ø­Ù…Ø²Ø© Ø®Ø§Ø´Ù‚Ø¬ÙŠ (13 Ø£ÙƒØªÙˆØ¨Ø± 1958ØŒ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©...   \n",
            "3  Ø¬Ù…Ø§Ù„ Ø£Ø­Ù…Ø¯ Ø­Ù…Ø²Ø© Ø®Ø§Ø´Ù‚Ø¬ÙŠ (13 Ø£ÙƒØªÙˆØ¨Ø± 1958ØŒ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©...   \n",
            "4  Ø¬Ù…Ø§Ù„ Ø£Ø­Ù…Ø¯ Ø­Ù…Ø²Ø© Ø®Ø§Ø´Ù‚Ø¬ÙŠ (13 Ø£ÙƒØªÙˆØ¨Ø± 1958ØŒ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©...   \n",
            "\n",
            "                                          question  \\\n",
            "0                   - Ù…Ù† Ù‡Ùˆ Ø¬Ù…Ø§Ù„ Ø£Ø­Ù…Ø¯ Ø­Ù…Ø²Ø© Ø®Ø§Ø´Ù‚Ø¬ÙŠØŸ   \n",
            "1        - Ù…ØªÙ‰ ÙˆÙ„Ø¯ Ø¬Ù…Ø§Ù„ Ø£Ø­Ù…Ø¯ Ø­Ù…Ø²Ø© Ø®Ø§Ø´Ù‚Ø¬ÙŠ ÙˆØªÙˆÙÙŠØŸ Ø§Ù„   \n",
            "2      - ÙÙŠ Ø£ÙŠ Ù…Ø¯ÙŠÙ†Ø© ÙˆÙ„Ø¯ Ø¬Ù…Ø§Ù„ Ø£Ø­Ù…Ø¯ Ø­Ù…Ø²Ø© Ø®Ø§Ø´Ù‚Ø¬ÙŠØŸ Ø§Ù„   \n",
            "3   - ÙÙŠ Ø£ÙŠ ØµØ­ÙŠÙØ© Ù‚Ø§Ù… Ø¨ÙƒØªØ§Ø¨Ø© Ø¹Ù…ÙˆØ¯ Ù…Ù†Ø° Ø¹Ø§Ù… 2017ØŸ Ø§Ù„   \n",
            "4  - ÙƒÙŠÙ ÙˆØµÙÙ‡Ø§ ÙÙŠ Ø§Ù„ØµØ­Ù ÙˆÙˆØ³Ø§Ø¦Ù„ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù… Ø§Ù„Ø¯ÙˆÙ„ÙŠØ©ØŸ Ø§Ù„   \n",
            "\n",
            "                                             answers  \n",
            "0  {'text': array(['ØµØ­ÙÙŠ ÙˆØ¥Ø¹Ù„Ø§Ù…ÙŠ'], dtype=object)...  \n",
            "1  {'text': array(['Ø­Ù…Ø²Ø© Ø®Ø§Ø´Ù‚Ø¬ÙŠ (13 Ø£ÙƒØªÙˆØ¨Ø± 1958ØŒ ...  \n",
            "2  {'text': array(['Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ù…Ù†ÙˆØ±Ø©'], dtype=obje...  \n",
            "3  {'text': array(['ÙˆØ§Ø´Ù†Ø·Ù† Ø¨ÙˆØ³Øª'], dtype=object),...  \n",
            "4  {'text': array(['ÙˆÙØµÙ ÙÙŠ Ø§Ù„ØµØ­Ù ÙˆØ£Ø¬Ù‡Ø²Ø© Ø§Ù„Ø§Ø¹Ù„Ø§Ù… ...  \n",
            "0      Ø¬Ù…Ø§Ù„ Ø£Ø­Ù…Ø¯ Ø­Ù…Ø²Ø© Ø®Ø§Ø´Ù‚Ø¬ÙŠ (13 Ø£ÙƒØªÙˆØ¨Ø± 1958ØŒ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©...\n",
            "1      Ø¬Ù…Ø§Ù„ Ø£Ø­Ù…Ø¯ Ø­Ù…Ø²Ø© Ø®Ø§Ø´Ù‚Ø¬ÙŠ (13 Ø£ÙƒØªÙˆØ¨Ø± 1958ØŒ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©...\n",
            "2      Ø¬Ù…Ø§Ù„ Ø£Ø­Ù…Ø¯ Ø­Ù…Ø²Ø© Ø®Ø§Ø´Ù‚Ø¬ÙŠ (13 Ø£ÙƒØªÙˆØ¨Ø± 1958ØŒ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©...\n",
            "3      Ø¬Ù…Ø§Ù„ Ø£Ø­Ù…Ø¯ Ø­Ù…Ø²Ø© Ø®Ø§Ø´Ù‚Ø¬ÙŠ (13 Ø£ÙƒØªÙˆØ¨Ø± 1958ØŒ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©...\n",
            "4      Ø¬Ù…Ø§Ù„ Ø£Ø­Ù…Ø¯ Ø­Ù…Ø²Ø© Ø®Ø§Ø´Ù‚Ø¬ÙŠ (13 Ø£ÙƒØªÙˆØ¨Ø± 1958ØŒ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©...\n",
            "                             ...                        \n",
            "688    Ø§Ù„Ø¨Ø¸Ø± Ù‡Ùˆ Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ù…Ø«ÙŠØ±Ø© Ù„Ù„Ø´Ù‡ÙˆØ© Ø§Ù„Ø¬Ù†Ø³ÙŠØ© Ø§Ù„Ø£ÙƒØ«Ø±...\n",
            "689    Ø§Ù„Ø¨Ø¸Ø± Ù‡Ùˆ Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ù…Ø«ÙŠØ±Ø© Ù„Ù„Ø´Ù‡ÙˆØ© Ø§Ù„Ø¬Ù†Ø³ÙŠØ© Ø§Ù„Ø£ÙƒØ«Ø±...\n",
            "690    ÙŠÙ‚ÙˆÙ„ Ù‚Ø§Ù…ÙˆØ³ Ø¥ÙƒØ³ÙÙˆØ±Ø¯ Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠÙ‘Ø© Ø£Ù† Ù„ÙƒÙ„Ù…Ø© \"...\n",
            "691    ÙŠÙ‚ÙˆÙ„ Ù‚Ø§Ù…ÙˆØ³ Ø¥ÙƒØ³ÙÙˆØ±Ø¯ Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠÙ‘Ø© Ø£Ù† Ù„ÙƒÙ„Ù…Ø© \"...\n",
            "692    ÙŠÙ‚ÙˆÙ„ Ù‚Ø§Ù…ÙˆØ³ Ø¥ÙƒØ³ÙÙˆØ±Ø¯ Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠÙ‘Ø© Ø£Ù† Ù„ÙƒÙ„Ù…Ø© \"...\n",
            "Name: text, Length: 693, dtype: object\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "data = pd.read_csv(\"train.csv\")\n",
        "#pre processing\n",
        "del data[\"id\"]\n",
        "del data[\"title\"]\n",
        "\n",
        "print ( data.head() )\n",
        "print ( data[\"text\"]  )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of question and answers: \", len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2T2ZHKDzF8oB",
        "outputId": "2987b4ad-d98a-4205-dccf-b09ee47db5f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of question and answers:  693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the Chatbot\n",
        "The best part about using these pre-trained models is that you can load the model and its tokenizer in just two simple lines of code. ğŸ˜² Isnâ€™t it simply wow? For tasks like text classification, we need to fine-tune BERT on our dataset. But for question answering tasks, we can even use the already trained model and get decent results even when our text is from a completely different domain. To get decent results, we are using a BERT model which is fine-tuned on the SQuAD benchmark.\n",
        "\n",
        "For our task, we will use the BertForQuestionAnswering class from the transformers library."
      ],
      "metadata": {
        "id": "oJhzltZNMtjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForQuestionAnswering.from_pretrained('asafaya/bert-base-arabic')\n",
        "tokenizer = BertTokenizer.from_pretrained('asafaya/bert-base-arabic')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRf_6bETGyJt",
        "outputId": "c89dec63-e8dd-4fff-99d6-b3ce84463c23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Asking a Question\n",
        "Letâ€™s randomly select a question number."
      ],
      "metadata": {
        "id": "PbX1sBBjMwbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_num = np.random.randint(0,len(data))\n",
        "question = data[\"question\"][random_num]\n",
        "text = data[\"text\"][random_num]"
      ],
      "metadata": {
        "id": "xjMfo6AGMz-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Letâ€™s tokenize the question and text as a pair."
      ],
      "metadata": {
        "id": "9l7iisiNM-jS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.encode(question, text)\n",
        "print(\"The input has a total of {} tokens.\".format(len(input_ids)))\n",
        "print ( input_ids )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbivNuCGM_1t",
        "outputId": "8f96e49b-22cd-45b2-ecef-3186eeaf6df9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input has a total of 162 tokens.\n",
            "[2, 1809, 2105, 1947, 6830, 16683, 2083, 1833, 4650, 1725, 4871, 2086, 4901, 3799, 2083, 2145, 221, 3, 3799, 2083, 2145, 2105, 7806, 15748, 15070, 1726, 18099, 3799, 2083, 29605, 1021, 5386, 3137, 219, 11165, 1960, 2369, 6230, 3260, 3188, 5620, 3955, 219, 2269, 15669, 2095, 4052, 1725, 4304, 1809, 1914, 2523, 4022, 22969, 2832, 2635, 4089, 1726, 2091, 2145, 18, 2166, 17545, 4052, 7829, 4821, 1020, 6830, 7806, 1905, 3738, 3260, 3188, 5620, 3955, 24741, 2572, 1725, 22, 4301, 4221, 18, 2166, 17545, 4052, 24330, 1747, 23, 22481, 1719, 2179, 1855, 1008, 219, 2264, 15703, 1013, 1747, 22481, 8117, 1914, 7531, 16318, 1013, 4399, 219, 9698, 1936, 1013, 3749, 1747, 22481, 8117, 2364, 1914, 10922, 8845, 30068, 1006, 219, 10716, 8498, 14402, 2066, 31183, 1735, 18, 2526, 8144, 2608, 1726, 18099, 3799, 2083, 1833, 13015, 1725, 4871, 5044, 219, 3489, 2105, 7226, 2608, 1833, 17509, 2281, 14281, 5694, 12615, 7806, 1905, 4901, 3799, 2083, 7691, 2815, 13015, 1725, 5241, 18, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Letâ€™s see how many tokens this question and text pair have."
      ],
      "metadata": {
        "id": "1aPyZ1d4NOMz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To look at what our tokenizer is doing, letâ€™s just print out the tokens and their IDs."
      ],
      "metadata": {
        "id": "kqzJHATRNkM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "for token, id in zip(tokens, input_ids):\n",
        "    print('{:8}{:8,}'.format(token,id))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk0Z2hfYNsaz",
        "outputId": "66b3807a-50e4-4c27-985c-3e19a2817f56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]          2\n",
            "Ù…Ø§         1,809\n",
            "Ù‡ÙŠ         2,105\n",
            "Ø§Ø®Ø±        1,947\n",
            "Ø§Ø³ØªØ¶Ø§ÙØ©    6,830\n",
            "Ù„ÙƒØ§Ø³      16,683\n",
            "Ø§Ù„Ø¹Ø§Ù„Ù…     2,083\n",
            "Ø§Ù„ØªÙŠ       1,833\n",
            "ØªÙ…Øª        4,650\n",
            "ÙÙŠ         1,725\n",
            "Ø§ÙˆØ±ÙˆØ¨Ø§     4,871\n",
            "Ù‚Ø¨Ù„        2,086\n",
            "Ø¨Ø·ÙˆÙ„Ø©      4,901\n",
            "ÙƒØ§Ø³        3,799\n",
            "Ø§Ù„Ø¹Ø§Ù„Ù…     2,083\n",
            "2018       2,145\n",
            "ØŸ            221\n",
            "[SEP]          3\n",
            "ÙƒØ§Ø³        3,799\n",
            "Ø§Ù„Ø¹Ø§Ù„Ù…     2,083\n",
            "2018       2,145\n",
            "Ù‡ÙŠ         2,105\n",
            "Ø§Ù„Ø¨Ø·ÙˆÙ„Ø©    7,806\n",
            "Ø§Ù„Ø­Ø§Ø¯ÙŠØ©   15,748\n",
            "ÙˆØ§Ù„Ø¹Ø´Ø±ÙˆÙ†  15,070\n",
            "Ù…Ù†         1,726\n",
            "Ø¨Ø·ÙˆÙ„Ø§Øª    18,099\n",
            "ÙƒØ§Ø³        3,799\n",
            "Ø§Ù„Ø¹Ø§Ù„Ù…     2,083\n",
            "Ù„ÙØ±       29,605\n",
            "##Ù‚        1,021\n",
            "Ø§Ù„Ø±Ø¬Ø§Ù„     5,386\n",
            "Ø§Ù„ÙˆØ·Ù†ÙŠØ©    3,137\n",
            "ØŒ            219\n",
            "ÙˆØ§Ù„Ù…Ù‚     11,165\n",
            "##Ø§Ù…Ø©      1,960\n",
            "ØªØ­Øª        2,369\n",
            "Ø±Ø¹Ø§ÙŠØ©      6,230\n",
            "Ø§Ù„Ø§ØªØ­Ø§Ø¯    3,260\n",
            "Ø§Ù„Ø¯ÙˆÙ„ÙŠ     3,188\n",
            "Ù„ÙƒØ±Ø©       5,620\n",
            "Ø§Ù„Ù‚Ø¯Ù…      3,955\n",
            "ØŒ            219\n",
            "ÙˆÙ‚Ø¯        2,269\n",
            "Ø§Ø³ØªØ¶Ø§Ù    15,669\n",
            "##ØªÙ‡Ø§      2,095\n",
            "Ø±ÙˆØ³ÙŠØ§      4,052\n",
            "ÙÙŠ         1,725\n",
            "Ø§Ù„ÙØªØ±Ø©     4,304\n",
            "Ù…Ø§         1,809\n",
            "Ø¨ÙŠÙ†        1,914\n",
            "14         2,523\n",
            "ÙŠÙˆÙ†ÙŠÙˆ      4,022\n",
            "ÙˆÙ„Øº       22,969\n",
            "##Ø§ÙŠØ©      2,832\n",
            "15         2,635\n",
            "ÙŠÙˆÙ„ÙŠÙˆ      4,089\n",
            "Ù…Ù†         1,726\n",
            "Ø¹Ø§Ù…        2,091\n",
            "2018       2,145\n",
            ".             18\n",
            "Ø­ÙŠØ«        2,166\n",
            "Ø§Ø³ØªØ·Ø§Ø¹Øª   17,545\n",
            "Ø±ÙˆØ³ÙŠØ§      4,052\n",
            "Ø§Ù„ÙÙˆØ²      7,829\n",
            "Ø¨Ø´Ø±        4,821\n",
            "##Ù        1,020\n",
            "Ø§Ø³ØªØ¶Ø§ÙØ©    6,830\n",
            "Ø§Ù„Ø¨Ø·ÙˆÙ„Ø©    7,806\n",
            "Ø¨Ø¹Ø¯        1,905\n",
            "Ù‚Ø±Ø§Ø±       3,738\n",
            "Ø§Ù„Ø§ØªØ­Ø§Ø¯    3,260\n",
            "Ø§Ù„Ø¯ÙˆÙ„ÙŠ     3,188\n",
            "Ù„ÙƒØ±Ø©       5,620\n",
            "Ø§Ù„Ù‚Ø¯Ù…      3,955\n",
            "Ø§Ù„Ù…Ø¹Ù„Ù†    24,741\n",
            "Ø¹Ù†Ù‡        2,572\n",
            "ÙÙŠ         1,725\n",
            "2             22\n",
            "Ø¯ÙŠØ³Ù…Ø¨Ø±     4,301\n",
            "2010       4,221\n",
            ".             18\n",
            "Ø­ÙŠØ«        2,166\n",
            "Ø§Ø³ØªØ·Ø§Ø¹Øª   17,545\n",
            "Ø±ÙˆØ³ÙŠØ§      4,052\n",
            "Ø§Ù„ØªØºÙ„Ø¨    24,330\n",
            "Ø¹Ù„Ù‰        1,747\n",
            "3             23\n",
            "ØªØ±Ø´ÙŠØ­     22,481\n",
            "##Ø§Øª       1,719\n",
            "Ø¨Ø§Ù„Ù…       2,179\n",
            "##Ø¬Ù…       1,855\n",
            "##Ù„        1,008\n",
            "ØŒ            219\n",
            "ÙØª         2,264\n",
            "##ØºÙ„Ø¨     15,703\n",
            "##Øª        1,013\n",
            "Ø¹Ù„Ù‰        1,747\n",
            "ØªØ±Ø´ÙŠØ­     22,481\n",
            "Ù…Ø´ØªØ±Ùƒ      8,117\n",
            "Ø¨ÙŠÙ†        1,914\n",
            "Ø§Ø³Ø¨Ø§Ù†ÙŠØ§    7,531\n",
            "ÙˆØ§Ù„Ø¨Ø±     16,318\n",
            "##Øª        1,013\n",
            "##ØºØ§Ù„      4,399\n",
            "ØŒ            219\n",
            "ÙˆØªØº        9,698\n",
            "##Ù„Ø¨       1,936\n",
            "##Øª        1,013\n",
            "ÙƒØ°Ù„Ùƒ       3,749\n",
            "Ø¹Ù„Ù‰        1,747\n",
            "ØªØ±Ø´ÙŠØ­     22,481\n",
            "Ù…Ø´ØªØ±Ùƒ      8,117\n",
            "Ø§ÙŠØ¶Ø§       2,364\n",
            "Ø¨ÙŠÙ†        1,914\n",
            "Ù‡ÙˆÙ„Ù†Ø¯Ø§    10,922\n",
            "ÙˆØ¨Ù„        8,845\n",
            "##Ø¬ÙŠÙƒ     30,068\n",
            "##Ø§        1,006\n",
            "ØŒ            219\n",
            "ÙˆØªØ±       10,716\n",
            "##Ø´ÙŠØ­      8,498\n",
            "ÙˆØ­ÙŠØ¯      14,402\n",
            "Ù„Ø§Ù†        2,066\n",
            "##Ø¬Ù„Øª     31,183\n",
            "##Ø±Ø§       1,735\n",
            ".             18\n",
            "ÙˆÙ‡ÙŠ        2,526\n",
            "Ø§Ù„Ù†Ø³Ø®Ø©     8,144\n",
            "Ø§Ù„Ø§ÙˆÙ„Ù‰     2,608\n",
            "Ù…Ù†         1,726\n",
            "Ø¨Ø·ÙˆÙ„Ø§Øª    18,099\n",
            "ÙƒØ§Ø³        3,799\n",
            "Ø§Ù„Ø¹Ø§Ù„Ù…     2,083\n",
            "Ø§Ù„ØªÙŠ       1,833\n",
            "Ø§Ù‚ÙŠÙ…Øª     13,015\n",
            "ÙÙŠ         1,725\n",
            "Ø§ÙˆØ±ÙˆØ¨Ø§     4,871\n",
            "Ø§Ù„Ø´Ø±Ù‚ÙŠØ©    5,044\n",
            "ØŒ            219\n",
            "ÙˆÙƒØ°Ù„Ùƒ      3,489\n",
            "Ù‡ÙŠ         2,105\n",
            "Ø§Ù„Ù…Ø±Ø©      7,226\n",
            "Ø§Ù„Ø§ÙˆÙ„Ù‰     2,608\n",
            "Ø§Ù„ØªÙŠ       1,833\n",
            "ØªØ³ØªØ¶ÙŠÙ    17,509\n",
            "Ø¨Ù‡Ø§        2,281\n",
            "Ø§Ù„Ù‚Ø§Ø±Ø©    14,281\n",
            "Ø§Ù„Ø§ÙˆØ±ÙˆØ¨ÙŠØ©   5,694\n",
            "Ù…Ù†Ø§ÙØ³Ø§Øª   12,615\n",
            "Ø§Ù„Ø¨Ø·ÙˆÙ„Ø©    7,806\n",
            "Ø¨Ø¹Ø¯        1,905\n",
            "Ø¨Ø·ÙˆÙ„Ø©      4,901\n",
            "ÙƒØ§Ø³        3,799\n",
            "Ø§Ù„Ø¹Ø§Ù„Ù…     2,083\n",
            "2006       7,691\n",
            "ÙˆØ§Ù„ØªÙŠ      2,815\n",
            "Ø§Ù‚ÙŠÙ…Øª     13,015\n",
            "ÙÙŠ         1,725\n",
            "Ø§Ù„Ù…Ø§Ù†ÙŠØ§    5,241\n",
            ".             18\n",
            "[SEP]          3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#first occurence of [SEP] token\n",
        "sep_idx = input_ids.index(tokenizer.sep_token_id)\n",
        "print(\"SEP token index: \", sep_idx)\n",
        "#number of tokens in segment A (question) - this will be one more than the sep_idx as the index in Python starts from 0\n",
        "num_seg_a = sep_idx+1\n",
        "print(\"Number of tokens in segment A: \", num_seg_a)\n",
        "#number of tokens in segment B (text)\n",
        "num_seg_b = len(input_ids) - num_seg_a\n",
        "print(\"Number of tokens in segment B: \", num_seg_b)\n",
        "#creating the segment ids\n",
        "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "#making sure that every input token has a segment id\n",
        "assert len(segment_ids) == len(input_ids)"
      ],
      "metadata": {
        "id": "Sy0Aj1uMO_Qr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70bcb638-bd37-44fb-c931-f622f314bd0f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SEP token index:  17\n",
            "Number of tokens in segment A:  18\n",
            "Number of tokens in segment B:  144\n"
          ]
        }
      ]
    }
  ]
}