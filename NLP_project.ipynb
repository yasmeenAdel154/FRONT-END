{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUkMcYmB4O6Qj79i/jZ3Nb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yasmeenAdel154/FRONT-END/blob/main/NLP_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**this code from : https://towardsdatascience.com/question-answering-with-a-fine-tuned-bert-bc4dafd45626**"
      ],
      "metadata": {
        "id": "oMAyddqSQCER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq2S3YD4D-9s",
        "outputId": "40da3356-9d8d-48c1-89f7-aa1c01eeb94e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Loading from Local CSV File"
      ],
      "metadata": {
        "id": "GKXHd8D3MZ-E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lda-Vu4kDcT1",
        "outputId": "86906af8-eb01-44f7-fc58-02db0938ddf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0  جمال أحمد حمزة خاشقجي (13 أكتوبر 1958، المدينة...   \n",
            "1  جمال أحمد حمزة خاشقجي (13 أكتوبر 1958، المدينة...   \n",
            "2  جمال أحمد حمزة خاشقجي (13 أكتوبر 1958، المدينة...   \n",
            "3  جمال أحمد حمزة خاشقجي (13 أكتوبر 1958، المدينة...   \n",
            "4  جمال أحمد حمزة خاشقجي (13 أكتوبر 1958، المدينة...   \n",
            "\n",
            "                                          question  \\\n",
            "0                   - من هو جمال أحمد حمزة خاشقجي؟   \n",
            "1        - متى ولد جمال أحمد حمزة خاشقجي وتوفي؟ ال   \n",
            "2      - في أي مدينة ولد جمال أحمد حمزة خاشقجي؟ ال   \n",
            "3   - في أي صحيفة قام بكتابة عمود منذ عام 2017؟ ال   \n",
            "4  - كيف وصفها في الصحف ووسائل الإعلام الدولية؟ ال   \n",
            "\n",
            "                                             answers  \n",
            "0  {'text': array(['صحفي وإعلامي'], dtype=object)...  \n",
            "1  {'text': array(['حمزة خاشقجي (13 أكتوبر 1958، ...  \n",
            "2  {'text': array(['المدينة المنورة'], dtype=obje...  \n",
            "3  {'text': array(['واشنطن بوست'], dtype=object),...  \n",
            "4  {'text': array(['وُصف في الصحف وأجهزة الاعلام ...  \n",
            "0      جمال أحمد حمزة خاشقجي (13 أكتوبر 1958، المدينة...\n",
            "1      جمال أحمد حمزة خاشقجي (13 أكتوبر 1958، المدينة...\n",
            "2      جمال أحمد حمزة خاشقجي (13 أكتوبر 1958، المدينة...\n",
            "3      جمال أحمد حمزة خاشقجي (13 أكتوبر 1958، المدينة...\n",
            "4      جمال أحمد حمزة خاشقجي (13 أكتوبر 1958، المدينة...\n",
            "                             ...                        \n",
            "688    البظر هو المنطقة المثيرة للشهوة الجنسية الأكثر...\n",
            "689    البظر هو المنطقة المثيرة للشهوة الجنسية الأكثر...\n",
            "690    يقول قاموس إكسفورد للغة الإنجليزيّة أن لكلمة \"...\n",
            "691    يقول قاموس إكسفورد للغة الإنجليزيّة أن لكلمة \"...\n",
            "692    يقول قاموس إكسفورد للغة الإنجليزيّة أن لكلمة \"...\n",
            "Name: text, Length: 693, dtype: object\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "data = pd.read_csv(\"train.csv\")\n",
        "#pre processing\n",
        "del data[\"id\"]\n",
        "del data[\"title\"]\n",
        "\n",
        "print ( data.head() )\n",
        "print ( data[\"text\"]  )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of question and answers: \", len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2T2ZHKDzF8oB",
        "outputId": "2987b4ad-d98a-4205-dccf-b09ee47db5f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of question and answers:  693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the Chatbot\n",
        "The best part about using these pre-trained models is that you can load the model and its tokenizer in just two simple lines of code. 😲 Isn’t it simply wow? For tasks like text classification, we need to fine-tune BERT on our dataset. But for question answering tasks, we can even use the already trained model and get decent results even when our text is from a completely different domain. To get decent results, we are using a BERT model which is fine-tuned on the SQuAD benchmark.\n",
        "\n",
        "For our task, we will use the BertForQuestionAnswering class from the transformers library."
      ],
      "metadata": {
        "id": "oJhzltZNMtjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForQuestionAnswering.from_pretrained('asafaya/bert-base-arabic')\n",
        "tokenizer = BertTokenizer.from_pretrained('asafaya/bert-base-arabic')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRf_6bETGyJt",
        "outputId": "c89dec63-e8dd-4fff-99d6-b3ce84463c23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Asking a Question\n",
        "Let’s randomly select a question number."
      ],
      "metadata": {
        "id": "PbX1sBBjMwbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_num = np.random.randint(0,len(data))\n",
        "question = data[\"question\"][random_num]\n",
        "text = data[\"text\"][random_num]"
      ],
      "metadata": {
        "id": "xjMfo6AGMz-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s tokenize the question and text as a pair."
      ],
      "metadata": {
        "id": "9l7iisiNM-jS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.encode(question, text)\n",
        "print(\"The input has a total of {} tokens.\".format(len(input_ids)))\n",
        "print ( input_ids )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbivNuCGM_1t",
        "outputId": "8f96e49b-22cd-45b2-ecef-3186eeaf6df9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input has a total of 162 tokens.\n",
            "[2, 1809, 2105, 1947, 6830, 16683, 2083, 1833, 4650, 1725, 4871, 2086, 4901, 3799, 2083, 2145, 221, 3, 3799, 2083, 2145, 2105, 7806, 15748, 15070, 1726, 18099, 3799, 2083, 29605, 1021, 5386, 3137, 219, 11165, 1960, 2369, 6230, 3260, 3188, 5620, 3955, 219, 2269, 15669, 2095, 4052, 1725, 4304, 1809, 1914, 2523, 4022, 22969, 2832, 2635, 4089, 1726, 2091, 2145, 18, 2166, 17545, 4052, 7829, 4821, 1020, 6830, 7806, 1905, 3738, 3260, 3188, 5620, 3955, 24741, 2572, 1725, 22, 4301, 4221, 18, 2166, 17545, 4052, 24330, 1747, 23, 22481, 1719, 2179, 1855, 1008, 219, 2264, 15703, 1013, 1747, 22481, 8117, 1914, 7531, 16318, 1013, 4399, 219, 9698, 1936, 1013, 3749, 1747, 22481, 8117, 2364, 1914, 10922, 8845, 30068, 1006, 219, 10716, 8498, 14402, 2066, 31183, 1735, 18, 2526, 8144, 2608, 1726, 18099, 3799, 2083, 1833, 13015, 1725, 4871, 5044, 219, 3489, 2105, 7226, 2608, 1833, 17509, 2281, 14281, 5694, 12615, 7806, 1905, 4901, 3799, 2083, 7691, 2815, 13015, 1725, 5241, 18, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s see how many tokens this question and text pair have."
      ],
      "metadata": {
        "id": "1aPyZ1d4NOMz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To look at what our tokenizer is doing, let’s just print out the tokens and their IDs."
      ],
      "metadata": {
        "id": "kqzJHATRNkM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "for token, id in zip(tokens, input_ids):\n",
        "    print('{:8}{:8,}'.format(token,id))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk0Z2hfYNsaz",
        "outputId": "66b3807a-50e4-4c27-985c-3e19a2817f56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]          2\n",
            "ما         1,809\n",
            "هي         2,105\n",
            "اخر        1,947\n",
            "استضافة    6,830\n",
            "لكاس      16,683\n",
            "العالم     2,083\n",
            "التي       1,833\n",
            "تمت        4,650\n",
            "في         1,725\n",
            "اوروبا     4,871\n",
            "قبل        2,086\n",
            "بطولة      4,901\n",
            "كاس        3,799\n",
            "العالم     2,083\n",
            "2018       2,145\n",
            "؟            221\n",
            "[SEP]          3\n",
            "كاس        3,799\n",
            "العالم     2,083\n",
            "2018       2,145\n",
            "هي         2,105\n",
            "البطولة    7,806\n",
            "الحادية   15,748\n",
            "والعشرون  15,070\n",
            "من         1,726\n",
            "بطولات    18,099\n",
            "كاس        3,799\n",
            "العالم     2,083\n",
            "لفر       29,605\n",
            "##ق        1,021\n",
            "الرجال     5,386\n",
            "الوطنية    3,137\n",
            "،            219\n",
            "والمق     11,165\n",
            "##امة      1,960\n",
            "تحت        2,369\n",
            "رعاية      6,230\n",
            "الاتحاد    3,260\n",
            "الدولي     3,188\n",
            "لكرة       5,620\n",
            "القدم      3,955\n",
            "،            219\n",
            "وقد        2,269\n",
            "استضاف    15,669\n",
            "##تها      2,095\n",
            "روسيا      4,052\n",
            "في         1,725\n",
            "الفترة     4,304\n",
            "ما         1,809\n",
            "بين        1,914\n",
            "14         2,523\n",
            "يونيو      4,022\n",
            "ولغ       22,969\n",
            "##اية      2,832\n",
            "15         2,635\n",
            "يوليو      4,089\n",
            "من         1,726\n",
            "عام        2,091\n",
            "2018       2,145\n",
            ".             18\n",
            "حيث        2,166\n",
            "استطاعت   17,545\n",
            "روسيا      4,052\n",
            "الفوز      7,829\n",
            "بشر        4,821\n",
            "##ف        1,020\n",
            "استضافة    6,830\n",
            "البطولة    7,806\n",
            "بعد        1,905\n",
            "قرار       3,738\n",
            "الاتحاد    3,260\n",
            "الدولي     3,188\n",
            "لكرة       5,620\n",
            "القدم      3,955\n",
            "المعلن    24,741\n",
            "عنه        2,572\n",
            "في         1,725\n",
            "2             22\n",
            "ديسمبر     4,301\n",
            "2010       4,221\n",
            ".             18\n",
            "حيث        2,166\n",
            "استطاعت   17,545\n",
            "روسيا      4,052\n",
            "التغلب    24,330\n",
            "على        1,747\n",
            "3             23\n",
            "ترشيح     22,481\n",
            "##ات       1,719\n",
            "بالم       2,179\n",
            "##جم       1,855\n",
            "##ل        1,008\n",
            "،            219\n",
            "فت         2,264\n",
            "##غلب     15,703\n",
            "##ت        1,013\n",
            "على        1,747\n",
            "ترشيح     22,481\n",
            "مشترك      8,117\n",
            "بين        1,914\n",
            "اسبانيا    7,531\n",
            "والبر     16,318\n",
            "##ت        1,013\n",
            "##غال      4,399\n",
            "،            219\n",
            "وتغ        9,698\n",
            "##لب       1,936\n",
            "##ت        1,013\n",
            "كذلك       3,749\n",
            "على        1,747\n",
            "ترشيح     22,481\n",
            "مشترك      8,117\n",
            "ايضا       2,364\n",
            "بين        1,914\n",
            "هولندا    10,922\n",
            "وبل        8,845\n",
            "##جيك     30,068\n",
            "##ا        1,006\n",
            "،            219\n",
            "وتر       10,716\n",
            "##شيح      8,498\n",
            "وحيد      14,402\n",
            "لان        2,066\n",
            "##جلت     31,183\n",
            "##را       1,735\n",
            ".             18\n",
            "وهي        2,526\n",
            "النسخة     8,144\n",
            "الاولى     2,608\n",
            "من         1,726\n",
            "بطولات    18,099\n",
            "كاس        3,799\n",
            "العالم     2,083\n",
            "التي       1,833\n",
            "اقيمت     13,015\n",
            "في         1,725\n",
            "اوروبا     4,871\n",
            "الشرقية    5,044\n",
            "،            219\n",
            "وكذلك      3,489\n",
            "هي         2,105\n",
            "المرة      7,226\n",
            "الاولى     2,608\n",
            "التي       1,833\n",
            "تستضيف    17,509\n",
            "بها        2,281\n",
            "القارة    14,281\n",
            "الاوروبية   5,694\n",
            "منافسات   12,615\n",
            "البطولة    7,806\n",
            "بعد        1,905\n",
            "بطولة      4,901\n",
            "كاس        3,799\n",
            "العالم     2,083\n",
            "2006       7,691\n",
            "والتي      2,815\n",
            "اقيمت     13,015\n",
            "في         1,725\n",
            "المانيا    5,241\n",
            ".             18\n",
            "[SEP]          3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#first occurence of [SEP] token\n",
        "sep_idx = input_ids.index(tokenizer.sep_token_id)\n",
        "print(\"SEP token index: \", sep_idx)\n",
        "#number of tokens in segment A (question) - this will be one more than the sep_idx as the index in Python starts from 0\n",
        "num_seg_a = sep_idx+1\n",
        "print(\"Number of tokens in segment A: \", num_seg_a)\n",
        "#number of tokens in segment B (text)\n",
        "num_seg_b = len(input_ids) - num_seg_a\n",
        "print(\"Number of tokens in segment B: \", num_seg_b)\n",
        "#creating the segment ids\n",
        "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "#making sure that every input token has a segment id\n",
        "assert len(segment_ids) == len(input_ids)"
      ],
      "metadata": {
        "id": "Sy0Aj1uMO_Qr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70bcb638-bd37-44fb-c931-f622f314bd0f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SEP token index:  17\n",
            "Number of tokens in segment A:  18\n",
            "Number of tokens in segment B:  144\n"
          ]
        }
      ]
    }
  ]
}